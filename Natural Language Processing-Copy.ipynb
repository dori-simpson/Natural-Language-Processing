{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8266e0b-52ce-4889-bd55-8fc1b2ca90a7",
   "metadata": {},
   "source": [
    "## Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9290609b-6e39-4935-b87e-773092c0348d",
   "metadata": {},
   "source": [
    "## 1. Reading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02815096-a017-44dd-8bff-a9067f1dc16b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install \"scikit-learn<1.7,>=1.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf4a070-6c03-4b1e-904e-87bcb8fef479",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"AMAZON-REVIEW-DATA-CLASSIFICATION.csv\")\n",
    "print('The shape of the dataset is:', df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aceed703-c4ea-4fd8-b4ad-64e146b9f9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4eeeab-c334-4325-b13f-f74bf4f8fed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47e6868-989a-4e82-b435-8c082d0e3ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.loc[[580]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef53b7c5-41a6-4346-8175-59cd34ffbf08",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e671495-485b-477d-8262-a6183df6839b",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34532d5-390d-40db-a32f-c1a43236e1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['isPositive'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6739e6d-d6ee-44ee-b8e2-8f516f5ae61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace({0:1, 1:0})\n",
    "df['isPositive'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7d8d61-aef6-4837-9f8d-9b92223f711a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6129820a-24f2-419e-afe9-84e891589a0d",
   "metadata": {},
   "source": [
    "## 3. Text Processing: Stop words removal and stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92115abf-4012-4284-be7e-f2ebef8b8f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the library and functions\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183028f0-ba2d-40b9-98ff-5cdf189d7ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Stop words\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "# Words we want to keep\n",
    "excluding = [\n",
    "    'against', 'not', 'don', \"don't\", 'ain', 'are', \"aren't\", 'could', \"couldn't\",\n",
    "    'did', \"didn't\", 'does', \"doesn't\", 'had', \"hadn't\", 'has', \"hasn't\",\n",
    "    'have', \"haven't\", 'is', \"isn't\", 'might', \"mightn't\", 'must', \"mustn't\",\n",
    "    'need', \"needn't\", 'should', \"shouldn't\", 'was', \"wasn't\", 'were', \"weren't\",\n",
    "    \"won't\", 'would', \"wouldn't\"\n",
    "]\n",
    "\n",
    "# Filtered stop words\n",
    "stop_words = [word for word in stop if word not in excluding]\n",
    "\n",
    "# Stemmer\n",
    "snow = SnowballStemmer('english')\n",
    "\n",
    "# Function to clean text\n",
    "def process_text(texts):\n",
    "    final_text_list = []\n",
    "\n",
    "    for sent in texts:\n",
    "        # Handle missing values\n",
    "        if not isinstance(sent, str):\n",
    "            sent = ''\n",
    "\n",
    "        filtered_sentence = []\n",
    "\n",
    "        # Basic cleaning\n",
    "        sent = sent.lower()\n",
    "        sent = sent.strip()\n",
    "        sent = re.sub(r'\\s+', ' ', sent)\n",
    "        sent = re.compile(r'<.*?>').sub('', sent)\n",
    "\n",
    "        # Tokenization and filtering\n",
    "        for w in word_tokenize(sent):\n",
    "            if (not w.isnumeric()) and (len(w) > 2) and (w not in stop_words):\n",
    "                filtered_sentence.append(snow.stem(w))\n",
    "\n",
    "        # Join tokens back into a string\n",
    "        final_string = \" \".join(filtered_sentence)\n",
    "        final_text_list.append(final_string)\n",
    "\n",
    "    return final_text_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d654cb1f-acca-4cf2-99e7-3137f4901a88",
   "metadata": {},
   "source": [
    "## 4. Training, Validation, and Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6dca067-f633-41d8-ae81-3fcb5f8c85aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# First split: train and temp (validation + test)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    df[['reviewText', 'summary', 'time', 'log_votes']],  # Features\n",
    "    df['isPositive'],                                     # Target\n",
    "    test_size=0.20,                                       # 20% for validation + test\n",
    "    shuffle=True,\n",
    "    random_state=324\n",
    ")\n",
    "\n",
    "# Second split: validation and test from temp\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_val,       # Features from previous split\n",
    "    y_val,       # Target from previous split\n",
    "    test_size=0.5,  # Split 50-50 to get equal validation and test sets\n",
    "    shuffle=True,\n",
    "    random_state=324\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9425df-53fd-4d72-a42e-09465995e12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Processing the reviewText fields')\n",
    "X_train['reviewText'] = process_text(X_train['reviewText'].tolist())\n",
    "X_val['reviewText'] = process_text(X_val['reviewText'].tolist())\n",
    "X_test['reviewText'] = process_text(X_test['reviewText'].tolist())\n",
    "\n",
    "print('Processing the summary fields')\n",
    "X_train['summary'] = process_text(X_train['summary'].tolist())\n",
    "X_val['summary'] = process_text(X_val['summary'].tolist())\n",
    "X_test['summary'] = process_text(X_test['summary'].tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343bcc92-493c-47e4-b723-0a26ff06ea19",
   "metadata": {},
   "source": [
    "## 5. Data processing with Pipeline and ColumnTransform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4049da68-90a4-45fa-b17c-0389e9683d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab model features/inputs and target/output\n",
    "numerical_features = ['time', 'log_votes']\n",
    "text_features = ['summary', 'reviewText']\n",
    "model_features = numerical_features + text_features\n",
    "model_target = 'isPositive'\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "### COLUMN TRANSFORMER ###\n",
    "##########################\n",
    "\n",
    "# Preprocess the numerical features\n",
    "numerical_processor = Pipeline([\n",
    "    ('num_imputer', SimpleImputer(strategy='mean')),\n",
    "    ('num_scaler', MinMaxScaler())\n",
    "])\n",
    "\n",
    "# Preprocess 1st text feature (summary)\n",
    "text_processor_0 = Pipeline([\n",
    "    ('text_vect_0', CountVectorizer(binary=True, max_features=50))\n",
    "])\n",
    "\n",
    "# Preprocess 2nd text feature (reviewText)\n",
    "text_processor_1 = Pipeline([\n",
    "    ('text_vect_1', CountVectorizer(binary=True, max_features=150))\n",
    "])\n",
    "\n",
    "# Combine all data preprocessors\n",
    "# Each processor: name, pipeline, and features to process\n",
    "data_preprocessor = ColumnTransformer([\n",
    "    ('numerical_pre', numerical_processor, numerical_features),\n",
    "    ('text_pre_0', text_processor_0, text_features[0]),\n",
    "    ('text_pre_1', text_processor_1, text_features[1])\n",
    "])\n",
    "\n",
    "### DATA PREPROCESSING ###\n",
    "##########################\n",
    "\n",
    "print(\n",
    "    'Datasets shapes before processing: ',\n",
    "    X_train.shape,\n",
    "    X_val.shape,\n",
    "    X_test.shape\n",
    ")\n",
    "\n",
    "# Fit and transform the training set, transform validation and test sets\n",
    "X_train = data_preprocessor.fit_transform(X_train).toarray()\n",
    "X_val = data_preprocessor.transform(X_val).toarray()\n",
    "X_test = data_preprocessor.transform(X_test).toarray()\n",
    "\n",
    "print(\n",
    "    'Datasets shapes after processing: ',\n",
    "    X_train.shape,\n",
    "    X_val.shape,\n",
    "    X_test.shape\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac691e7-09e7-4c6d-9efe-53322597b46e",
   "metadata": {},
   "source": [
    "## 6. Train a classifier and build algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5dd6a7-d592-4e66-bb21-0a70fb993435",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b64437a-7634-4bc7-a9dc-73f13003b07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define the XGBoost classifier\n",
    "linear_classifier = xgb.XGBClassifier(\n",
    "    objective='binary:logistic',  # for binary classification\n",
    "    n_estimators=100,              # number of boosting rounds\n",
    "    max_depth=5,                   # maximum tree depth\n",
    "    learning_rate=0.1,             # step size shrinkage\n",
    "    subsample=0.8,                 # fraction of samples per tree\n",
    "    colsample_bytree=0.8,          # fraction of features per tree\n",
    "    random_state=324               # for reproducibility\n",
    ")\n",
    "\n",
    "# Fit the model on training data\n",
    "linear_classifier.fit(\n",
    "    X_train.astype('float32'),\n",
    "    y_train.values.astype('float32')\n",
    ")\n",
    "\n",
    "# Predict on validation and test sets\n",
    "y_val_pred = linear_classifier.predict(X_val.astype('float32'))\n",
    "y_test_pred = linear_classifier.predict(X_test.astype('float32'))\n",
    "\n",
    "# Evaluate accuracy\n",
    "val_acc = accuracy_score(y_val, y_val_pred)\n",
    "test_acc = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print(\"Validation accuracy:\", val_acc)\n",
    "print(\"Test accuracy:\", test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13045d6-b035-42f6-a89d-fdb611c41f6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
